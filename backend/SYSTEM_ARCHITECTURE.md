# Zendroid Technical Architecture

This document provides a highly accurate technical specification of the Zendroid mobile automation system. It details the component architecture, data flow, state management, and interaction protocols.

## 1. System Components

The system is architected as a decoupled 4-tier stack:

- **Orchestration Layer (`AgentRunner`)**: Managing the lifecycle of a single test run. It serves as the bridge between WebSockets, the LangGraph workflow, and the LLM brain.
- **Intelligence Layer (`GeminiClient`)**: Using `gemini-2.5-flash` for vision-based navigation, task planning, and native tool-use interaction.
- **Workflow Layer (`WorkflowManager`)**: A LangGraph `StateGraph` implementation that enforces the logic of mobile automation.
- **Automation Layer (`AppiumDriver`)**: A wrapper around the Appium UiAutomator2 driver, handling physical device interaction and coordinate transformation.

---

## 2. Global Execution Flow

1.  **Session Creation**: The browser sends a `POST /test/start`. The backend instantiates an `AgentRunner` and stores it in an in-memory `active_agents` dictionary.
2.  **Socket Connection**: The browser upgrades to a WebSocket. The backend triggers `agent.run()` and starts a concurrent listener for user messages.
3.  **Appium Boot**: The `AppiumDriver` initializes a session with the Android device via the provided APK path.
4.  **Graph Execution**: The `AgentRunner` starts an asynchronous stream (`astream`) of the LangGraph workflow.
5.  **The Feedback Loop**:
    - **Planner**: Generates a discrete list of tasks.
    - **Navigator**: Captures a screenshot and queries the LLM for the next precise action (tap, type, swipe).
    - **Executor**: Translates the LLM action into physical device commands.
    - **Routing**: The graph loops between Navigation and Execution until the plan is complete or a failure is detected.

---

## 3. LangGraph Orchestration

### 3.1 AgentState Definition
The system state is defined by the `AgentState` TypedDict:
- `run_id`: Unique session identifier.
- `goal`: High-level user objective.
- `plan`: List of task-level strings generated by the Planner.
- `current_step_index`: Integer tracking progress through the `plan`.
- `history`: List of dictionaries (system messages and assistant actions) managed via `operator.add` for cumulative updates.
- `screenshot`: Base64 encoded PNG of the current device screen.
- `last_action`: The most recently predicted action JSON.
- `status`: Lifecycle string (`running`, `failed`, `passed`).
- `messages`: List of events dispatched to the frontend.

### 3.2 Graph Nodes
- **`planner`**: Purely cognitive; decomposes the goal into a task list.
- **`navigator`**: Vision-centric; captures a screenshot and uses the LLM to analyze the screen relative to the current task in the plan.
- **`executor`**: Operational; performs the action provided by the navigator and increments the step index if a task is marked `done`.

### 3.3 Conditional Routing
The `should_continue` function evaluates the state after each navigation step:
- `failed` status or `fail` action -> `END`.
- `done` action and last step reached -> `END`.
- `done` action but steps remaining -> Loop back to `executor` (to verify next state).
- Continuous actions -> `execute`.

---

## 4. Automation and Signal Calibration

### 4.1 Coordinate Scaling
There is a discrepancy between LLM vision and Appium interaction on some high-density devices:
- **Vision Space**: Physical pixels (e.g., 1080x2400) from high-res screenshots.
- **Action Space**: Logical points (e.g., 360x800) used by the Appium driver.

The `AppiumDriver._get_scale_ratio` method handles this by:
1.  Fetching the current window size (logical).
2.  Fetching the current screenshot size (physical).
3.  Calculating a `scale_ratio` for both X and Y axes.
4.  Applying this ratio to every `tap` and `swipe` command to translate physical pixels to logical points.

### 4.2 Action Implementation
- **Tap**: Uses W3C standard `ActionBuilder` with `PointerInput(interaction.POINTER_TOUCH, "touch")` after coordinate transformation.
- **Type**: Uses `ActionBuilder.key_action` with a fallback to `active_element.send_keys`.
- **Swipe**: Performs a sequence of `move_to`, `pointer_down`, `pause`, `move_to` (target), and `pointer_up` using scaled coordinates.

---

## 5. Conversational Interaction (Tools)

User messages are handled via an agentic supervision layer using **Gemini Tool Use**:

1.  User sends text via WebSocket.
2.  `AgentRunner.handle_user_input` queries Gemini via `llm.interpret_chat`.
3.  The model is provided with native tool definitions:
    - `stop_agent`
    - `restart_agent`
    - `update_goal(new_goal)`
    - `reply_to_user(message)`
4.  The `GeminiClient` returns the structured Tool Call.
5.  `AgentRunner` dispatches the call to private handler methods (`_tool_*`) using a `tool_dispatcher` map. This allows for dynamic agent control (like changing mid-test goals) without nested conditional logic.

---

## 6. Communication Protocol

All communication between Backend and Frontend occurs over JSON-encoded WebSocket messages with a `type` discriminator:
- `status`: Text updates for the progress bar.
- `screenshot`: Live visual feed.
- `plan`: The list of tasks generated.
- `action_plan`: The LLM's reasoning and intended next action.
- `action_executed`: Confirmation of physical input.
- `chat_response`: Direct feedback from the agent during conversation.
- `complete`: Final status payload.
